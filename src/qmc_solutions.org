#+TITLE: QMC-LTTC 2023 Exercises
#+AUTHOR: Vijay Gopal Chilkuri
#+LANGUAGE:  en
#+INFOJS_OPT: toc:t mouse:underline path:org-info.js
#+STARTUP: latexpreview
#+LATEX_CLASS: article
#+LATEX_HEADER_EXTRA: \usepackage{minted}
#+HTML_HEAD: <link rel="stylesheet" title="Standard" href="worg.css" type="text/css" />

#+OPTIONS: H:4 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS: TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
# EXCLUDE_TAGS: solution solution2 noexport
# EXCLUDE_TAGS: solution noexport
#+EXCLUDE_TAGS: noexport

  #+BEGIN_SRC elisp :output none :exports none
(setq org-latex-listings 'minted
      org-latex-packages-alist '(("" "minted"))
      org-latex-pdf-process
      '("pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
(setq org-latex-minted-options '(("breaklines" "true")
                                 ("breakanywhere" "true")))
(setq org-latex-minted-options
      '(("frame" "lines")
        ("fontsize" "\\scriptsize")
        ("linenos" "")))
(org-beamer-export-to-pdf)

  #+END_SRC

  #+RESULTS:
  : /home/chilkuri/Documents/codes/julia/qmc-lttc-2023/qmc_solutions.pdf



* Numerical evaluation of the energy of the hydrogen atom

** Local energy
   :PROPERTIES:
   :header-args:julia: :tangle hydrogen.jl
   :END:

*** Exercise 1

    #+begin_exercise
    Write a function which computes the potential at $\mathbf{r}$.
    The function accepts a 3-dimensional vector =r= as input argument
    and returns the potential.
    #+end_exercise

    $\mathbf{r}=\left( \begin{array}{c} x \\ y\\ z\end{array} \right)$, so
    $$
    V(\mathbf{r}) = -\frac{1}{\sqrt{x^2 + y^2 + z^2}}
    $$


**** Solution                                                      :solution:
    *Julia*
     #+BEGIN_SRC julia :results none
using LinearAlgebra

function potential(r)
    nr = norm(r);
    @assert (nr > 0.0);
    return(-1.0/nr)
end
     #+END_SRC

*** Exercise 2
    #+begin_exercise
    Write a function which computes the wave function at $\mathbf{r}$.
    The function accepts a scalar =a= and a 3-dimensional vector =r= as
    input arguments, and returns a scalar.
    #+end_exercise

**** Solution                                                      :solution:
    *Julia*
     #+BEGIN_SRC julia :results none
function psi(a, r)
    return exp(-a*norm(r))
end
     #+END_SRC
*** Exercise 3
    #+begin_exercise
    Write a function which computes the local kinetic energy at $\mathbf{r}$.
    The function accepts =a= and =r= as input arguments and returns the
    local kinetic energy.
    #+end_exercise

    The local kinetic energy is defined as $$T_L(\mathbf{r}) = -\frac{1}{2}\frac{\Delta \Psi(\mathbf{r})}{\Psi(\mathbf{r})}.$$

    We differentiate $\Psi$ with respect to $x$:

    \[ \Psi(\mathbf{r})  =  \exp(-a\,|\mathbf{r}|) \]
    \[\frac{\partial \Psi}{\partial x}
      = \frac{\partial \Psi}{\partial |\mathbf{r}|} \frac{\partial |\mathbf{r}|}{\partial x}
      =  - \frac{a\,x}{|\mathbf{r}|} \Psi(\mathbf{r}) \]

    and we differentiate a second time:

    $$
    \frac{\partial^2 \Psi}{\partial x^2} =
    \left( \frac{a^2\,x^2}{|\mathbf{r}|^2}  -
    \frac{a(y^2+z^2)}{|\mathbf{r}|^{3}} \right) \Psi(\mathbf{r}).
    $$

    The Laplacian operator $\Delta = \frac{\partial^2}{\partial x^2} +
    \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}$
    applied to the wave function gives:

    $$
    \Delta \Psi (\mathbf{r}) = \left(a^2 - \frac{2a}{\mathbf{|r|}} \right) \Psi(\mathbf{r})\,.
    $$

    Therefore, the local kinetic energy is
    $$
    T_L (\mathbf{r}) = -\frac{1}{2}\left(a^2 - \frac{2a}{\mathbf{|r|}} \right)
    $$

    *Julia*
     #+BEGIN_SRC julia :results none
function kinetic(a,r)
    nr = norm(r);
    @assert (nr > 0.0);
    return(-0.5 * (a*a - 2.0*a/nr))
end
     #+END_SRC

*** Exercise 4
    #+begin_exercise
    Write a function which computes the local energy at $\mathbf{r}$,
    using the previously defined functions.
    The function accepts =a= and =r= as input arguments and returns the
    local kinetic energy.
    #+end_exercise

    $$
    E_L(\mathbf{r}) = -\frac{1}{2} \frac{\Delta \Psi}{\Psi} (\mathbf{r}) + V(\mathbf{r})
    $$


    *Julia*
     #+BEGIN_SRC julia :results none
function e_loc(a,r)
    epot = potential(r);
    ekin = kinetic(a,r);
    return( ekin + epot )
end
     #+END_SRC

** Plot of the local energy along the $x$ axis
   :PROPERTIES:
   :header-args:julia: :tangle plot_hydrogen.jl
   :END:

*** Exercise

    #+begin_exercise
    For multiple values of $a$ (0.1, 0.2, 0.5, 1., 1.5, 2.), plot the
    local energy along the $x$ axis. In Python, you can use matplotlib
    for example. In Fortran, it is convenient to write in a text file
    the values of $x$ and $E_L(\mathbf{r})$ for each point, and use
    Gnuplot to plot the files. With Gnuplot, you will need 2 blank
    lines to separate the data corresponding to different values of $a$.
    #+end_exercise

   #+begin_note
   The potential and the kinetic energy both diverge at $r=0$, so we
   choose a grid which does not contain the origin to avoid numerical issues.
   #+end_note

**** Solution                                                      :solution:
    *Julia*
     #+BEGIN_SRC julia :results none
using LinearAlgebra
using Plots

include("hydrogen.jl")

x=range(-5,5,length=100)
y=x .|> y->e_loc(0.1,y)
plt = plot(x,y,label="a=0.1")

for a in [0.2, 0.5, 1., 1.5, 2.]
  y=x .|> y->e_loc(a,y)
  plot!(x,y,label="a=$(a)");
end

savefig(plt, "plot_py.png")
     #+end_src
** Numerical estimation of the energy
   :PROPERTIES:
   :header-args:julia: :tangle energy_hydrogen.jl
   :END:

*** Exercise
     #+begin_exercise
    Compute a numerical estimate of the energy using a grid of
    $50\times50\times50$ points in the range $(-5,-5,-5) \le
    \mathbf{r} \le (5,5,5)$.
     #+end_exercise
**** Solution                                                      :solution:
    *Julia*
     #+BEGIN_SRC julia :results output :exports both
using LinearAlgebra
include("hydrogen.jl");


interval = range(-5,5,length=50);
delta = (interval[2]-interval[1])^3;

r = [0.,0.,0.];
for a in [0.1, 0.2, 0.5, 0.9, 1., 1.5, 2.]
    E    = 0.;
    norm = 0.;

    for x in interval
        r[1] = x;
        for y in interval
            r[2] = y;
            for z in interval
                r[3] = z;

                w = psi(a,r);
                w = w * w * delta;

                E    += w * e_loc(a,r);
                norm += w;
            end
        end
    end
    E = E / norm;
    println("a = $(a) \t E = $(E)")
end
     #+end_src

     #+RESULTS:
     : a = 0.1 	 E = -0.24518438948809218
     : a = 0.2 	 E = -0.26966057967803525
     : a = 0.5 	 E = -0.3856357612517401
     : a = 0.9 	 E = -0.49435709786716214
     : a = 1.0 	 E = -0.5
     : a = 1.5 	 E = -0.3924296708260237
     : a = 2.0 	 E = -0.08086980667845059

*** Exercise
   #+begin_exercise
   Add the calculation of the variance to the previous code, and
   compute a numerical estimate of the variance of the local energy using
   a grid of $50\times50\times50$ points in the range $(-5,-5,-5) \le
   \mathbf{r} \le (5,5,5)$ for different values of $a$.

   #+end_exercise
     *Julia*
     #+BEGIN_SRC julia :results output :exports both
using LinearAlgebra
include("hydrogen.jl");


interval = range(-5,5,length=50);
delta = (interval[2]-interval[1])^3;

r = [0.,0.,0.];

for a in [0.1, 0.2, 0.5, 0.9, 1., 1.5, 2.]
    E    = 0.;
    E2   = 0.;
    norm = 0.;

    for x in interval
        r[1] = x;
        for y in interval
            r[2] = y;
            for z in interval
                r[3] = z;

                w = psi(a,r);
                w = w * w * delta;

                E    += w * e_loc(a,r);
                E2   += w * e_loc(a,r) * e_loc(a,r);
                norm += w;
            end
        end
    end
    E = E / norm;
    E2= E2/ norm;
    var = E2 - E*E;
    println("a = $(a) \t E = $(E) \t σ^2 = $(var)")
end
     #+end_src

     #+RESULTS:
     : a = 0.1 	 E = -0.24518438948809218 	 σ^2 = 0.026965218719722663
     : a = 0.2 	 E = -0.26966057967803525 	 σ^2 = 0.037197072370200784
     : a = 0.5 	 E = -0.3856357612517401 	 σ^2 = 0.053185967578480015
     : a = 0.9 	 E = -0.49435709786716214 	 σ^2 = 0.005778118170656099
     : a = 1.0 	 E = -0.5 	 σ^2 = 0.0
     : a = 1.5 	 E = -0.3924296708260237 	 σ^2 = 0.3144967090917285
     : a = 2.0 	 E = -0.08086980667845059 	 σ^2 = 1.806881427084649

* Variational Monte Carlo
** Computation of the statistical error
   :PROPERTIES:
   :header-args:julia: :tangle qmc_stats.jl
   :END:

   To compute the statistical error, you need to perform $M$
   independent Monte Carlo calculations. You will obtain $M$ different
   estimates of the energy, which are expected to have a Gaussian
   distribution for large $M$, according to the [[https://en.wikipedia.org/wiki/Central_limit_theorem][Central Limit Theorem]].

   The estimate of the energy is

   $$
   E = \frac{1}{M} \sum_{i=1}^M E_i
   $$

   The variance of the average energies can be computed as

   $$
   \sigma^2 = \frac{1}{M-1} \sum_{i=1}^{M} (E_i - E)^2
   $$

   And the confidence interval is given by

   $$
   E \pm \delta E, \text{ where } \delta E = \frac{\sigma}{\sqrt{M}}
   $$
*** Exercise
   #+begin_exercise
   Write a function returning the average and statistical error of an
   input array.
   #+end_exercise
**** Solution                                                     :solution:
    *Julia*
     #+BEGIN_SRC julia :results none :exports code

function ave_error(arr)
    M = length(arr)
    @assert(M>0)

    if M == 1
        average = arr[1]
        error   = 0.

    else
        average = sum(arr)/M
        variance = 1.0/(M-1) * ( arr |> x-> ( x .- average ).^2 |> sum )
        error = sqrt(variance/M)
    end

    return (average, error)
end
     #+END_SRC

** Uniform sampling in the box
   :PROPERTIES:
   :header-args:julia: :tangle qmc_uniform.jl
   :END:

*** Exercise

   One Monte Carlo run will consist of $N_{\rm MC}$ Monte Carlo iterations. At every Monte Carlo iteration:

   - Draw a random point $\mathbf{r}_i$ in the box $(-5,-5,-5) \le
     (x,y,z) \le (5,5,5)$
   - Compute $|\Psi(\mathbf{r}_i)|^2$ and accumulate the result in a
     variable =normalization=
   - Compute $|\Psi(\mathbf{r}_i)|^2 \times E_L(\mathbf{r}_i)$, and accumulate the
     result in a variable =energy=

   Once all the iterations have been computed, the run returns the average energy
   $\bar{E}_k$ over the $N_{\rm MC}$ iterations of the run.

   To compute the statistical error, perform $M$ independent runs. The
   final estimate of the energy will be the average over the
   $\bar{E}_k$, and the variance of the $\bar{E}_k$ will be used to
   compute the statistical error.

    #+begin_exercise
    Parameterize the wave function with $a=1.2$.  Perform 30
    independent Monte Carlo runs ($M$), each with 100 000 Monte Carlo
    steps ($N_{MC}$). Store the final energies of each run and use this array to
    compute the average energy and the associated error bar ($\delta E$).

    #+end_exercise
**** Solution                                                     :solution:
    *Julia*
     #+BEGIN_SRC julia  :results output :exports both
include("hydrogen.jl");
include("qmc_stats.jl");

function MonteCarlo(a, nmax)
     energy = 0.;
     normalization = 0.;

     for istep in range(1,nmax)

         R = 5.0;
         phi = rand()*2*π;
         costheta = rand()*2 - 1.0;
         u = rand();

         theta = acos( costheta );
         r = R * cbrt( u );

         # Spherical distribution
         x = r * sin( theta) * cos( phi );
         y = r * sin( theta) * sin( phi );
         z = r * cos( theta );
         # Cuboidal distribution
         #x = 10 * rand() - 5.0;
         #y = 10 * rand() - 5.0;
         #z = 10 * rand() - 5.0;
         r = [x,y,z];

         w = psi(a,r);
         w = w*w;

         energy        += w * e_loc(a,r);
         normalization += w;
     end

     return energy / normalization
end

a    = 1.2;
nmax = 100_000;

X = [MonteCarlo(a,nmax) for i in range(1,30)];
E, deltaE = ave_error(X);

println("E = $(E) +/- $(deltaE)")
     #+END_SRC

     #+RESULTS:
     : Sphere
     : E = -0.48024356420973185 +/- 0.001321161390324179
     : Cube
     : E = -0.4787870280458861 +/- 0.0020675933699247735

** Metropolis sampling with $\Psi^2$
   :PROPERTIES:
   :header-args:julia: :tangle qmc_metropolis.jl
   :END:

   We will now use the square of the wave function to sample random
   points distributed with the probability density
   \[
   P(\mathbf{r}) = \frac{|\Psi(\mathbf{r})|^2}{\int |\Psi(\mathbf{r})|^2 d\mathbf{r}}\,.
   \]

   The expression of the average energy is now simplified as the average of
   the local energies, since the weights are taken care of by the
   sampling:

   $$
   E \approx \frac{1}{N_{\rm MC}}\sum_{i=1}^{N_{\rm MC}} E_L(\mathbf{r}_i)\,.
   $$

   To sample a chosen probability density, an efficient method is the
   [[https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm][Metropolis-Hastings sampling algorithm]]. Starting from a random
   initial position $\mathbf{r}_0$, we will realize a random walk:

   $$ \mathbf{r}_0 \rightarrow \mathbf{r}_1 \rightarrow \mathbf{r}_2 \ldots \rightarrow \mathbf{r}_{N_{\rm MC}}\,, $$

   according to the following algorithm.

   At every step, we propose a new move according to a transition probability $T(\mathbf{r}_{n}\rightarrow\mathbf{r}_{n+1})$ of our choice.

   For simplicity, we will move the electron in a 3-dimensional box of side $2\delta L$ centered at the current position
   of the electron:

   $$
   \mathbf{r}_{n+1} = \mathbf{r}_{n} + \delta L \, \mathbf{u}
   $$

   where $\delta L$ is a fixed constant, and
   $\mathbf{u}$ is a uniform random number in a 3-dimensional box
   $(-1,-1,-1) \le \mathbf{u} \le (1,1,1)$.

   After having moved the electron, we add the
   accept/reject step that guarantees that the distribution of the
   $\mathbf{r}_n$ is $\Psi^2$. This amounts to accepting the move with
   probability

   $$
   A(\mathbf{r}_{n}\rightarrow\mathbf{r}_{n+1}) = \min\left(1,\frac{T(\mathbf{r}_{n+1}\rightarrow\mathbf{r}_{n}) P(\mathbf{r}_{n+1})}{T(\mathbf{r}_{n}\rightarrow\mathbf{r}_{n+1})P(\mathbf{r}_{n})}\right)\,,
   $$

   which, for our choice of transition probability, becomes

   $$
   A(\mathbf{r}_{n}\rightarrow\mathbf{r}_{n+1}) = \min\left(1,\frac{P(\mathbf{r}_{n+1})}{P(\mathbf{r}_{n})}\right)= \min\left(1,\frac{|\Psi(\mathbf{r}_{n+1})|^2}{|\Psi(\mathbf{r}_{n})|^2}\right)\,.
   $$

   #+begin_exercise
   Explain why the transition probability cancels out in the
   expression of $A$.
   #+end_exercise
   Also note that we do not need to compute the norm of the wave function!

   The algorithm is summarized as follows:

   1) Evaluate the local energy at $\mathbf{r}_n$ and accumulate it
   2) Compute a new position $\mathbf{r'} = \mathbf{r}_n + \delta L\, \mathbf{u}$
   3) Evaluate $\Psi(\mathbf{r}')$ at the new position
   4) Compute the ratio $A = \frac{\left|\Psi(\mathbf{r'})\right|^2}{\left|\Psi(\mathbf{r}_{n})\right|^2}$
   5) Draw a uniform random number $v \in [0,1]$
   6) if $v \le A$, accept the move : set $\mathbf{r}_{n+1} = \mathbf{r'}$
   7) else, reject the move : set $\mathbf{r}_{n+1} = \mathbf{r}_n$

   #+begin_note
    A common error is to remove the rejected samples from the
    calculation of the average. *Don't do it!*

    All samples should be kept, from both accepted /and/ rejected moves.
   #+end_note


*** Optimal step size

    If the box is infinitely small, the ratio will be very close
    to one and all the steps will be accepted. However, the moves will be
    very correlated and you will explore the configurational space very slowly.

    On the other hand, if you propose too large moves, the number of
    accepted steps will decrease because the ratios might become
    small. If the number of accepted steps is close to zero, then the
    space is not well sampled either.

    The size of the move should be adjusted so that it is as large as
    possible, keeping the number of accepted steps not too small. To
    achieve that, we define the acceptance rate as the number of
    accepted steps over the total number of steps. Adjusting the time
    step such that the acceptance rate is close to 0.5 is a good
    compromise for the current problem.

   #+begin_note
    Below, we use the symbol $\delta t$ to denote $\delta L$ since we will use
    the same variable later on to store a time step.
   #+end_note


*** Exercise

    #+begin_exercise
    Modify the program of the previous section to compute the energy,
    sampled with $\Psi^2$.

    Compute also the acceptance rate, so that you can adapt the time
    step in order to have an acceptance rate close to 0.5.

    Can you observe a reduction in the statistical error?
    #+end_exercise
**** Solution                                                     :solution:
    *Julia*
     #+BEGIN_SRC julia :results output :exports both
include("hydrogen.jl")
include("qmc_stats.jl")

function MonteCarlo(a,nmax,dt)
    energy  = 0.;
    N_accep = 0;

    x_old = rand()*2*dt - dt;
    y_old = rand()*2*dt - dt;
    z_old = rand()*2*dt - dt;
    r_old = [x_old,y_old,z_old];
    psi_old = psi(a,r_old);

    for istep in range(1,nmax)
        energy += e_loc(a,r_old);

        x_new = rand()*2*dt - dt;
        y_new = rand()*2*dt - dt;
        z_new = rand()*2*dt - dt;
        r_new = [x_new,y_new,z_new];
        r_new = r_old + r_new;
        psi_new = psi(a,r_new);

        ratio = (psi_new / psi_old)^2;

        if rand() <= ratio
            N_accep += 1;

            r_old   = r_new;
            psi_old = psi_new;
        end
    end

    return([energy/nmax, N_accep/nmax])
end

# Run simulation
a    = 1.2
nmax = 100000
dt   = 1.0

X0 = foldl(hcat,1:30 .|> x->MonteCarlo(a,nmax,dt))

# Energy
X = X0[1,:];
E, deltaE = ave_error(X)
print("E = $(E) +/- $(deltaE)")

# Acceptance rate
X = X0[2,:];
A, deltaA = ave_error(X)
print("A = $(A) +/- $(deltaA)")
     #+END_SRC

     #+RESULTS:
     : E = -0.4799712012519308 +/- 0.0004972233203530915:
     : A = 0.5076869999999999 +/- 0.0003981466113606039

** Generalized Metropolis algorithm
   :PROPERTIES:
   :header-args:julia: :tangle vmc_metropolis.jl
   :END:

   One can use more efficient numerical schemes to move the electrons by
   choosing a smarter expression for the transition probability.

   The Metropolis acceptance step has to be adapted keeping in mind that
   the detailed balance condition is satisfied. This means that the acceptance
   probability $A$ is chosen so that it is consistent with the probability of
   leaving $\mathbf{r}_n$ and the probability of entering $\mathbf{r}_{n+1}$:

   \[
   P(\mathbf{r}_{n} \rightarrow \mathbf{r}_{n+1}) = A(\mathbf{r}_{n} \rightarrow \mathbf{r}_{n+1}) T(\mathbf{r}_{n} \rightarrow \mathbf{r}_{n+1})
   = A(\mathbf{r}_{n+1} \rightarrow \mathbf{r}_{n}) T(\mathbf{r}_{n+1} \rightarrow \mathbf{r}_{n})
   \frac{P(\mathbf{r}_{n+1})}{P(\mathbf{r}_{n})}
   \]

   where $T(\mathbf{r}_n \rightarrow \mathbf{r}_{n+1})$ is the
   probability of transition from $\mathbf{r}_n$ to
   $\mathbf{r}_{n+1}$ and $P(\mathbf{r}_n \rightarrow \mathbf{r}_{n+1})$ is the
   conditional probability $P(\mathbf{r}_n | \mathbf{r}_{n+1})$ and $P(\mathbf{r}_n)$
   is the probability of being in state $\mathbf{r}_n$.

   In the previous example, we were using uniform sampling in a box centered
   at the current position. Hence, the transition probability was symmetric

   \[
   T(\mathbf{r}_{n} \rightarrow \mathbf{r}_{n+1})  = T(\mathbf{r}_{n+1} \rightarrow \mathbf{r}_{n})
   = \text{constant}\,,
   \]

   so the expression of $A$ was simplified to the ratios of the squared
   wave functions.

   Now, if instead of drawing uniform random numbers, we
   choose to draw Gaussian random numbers with zero mean and variance
   $\delta t$, the transition probability becomes:

   \[
   T(\mathbf{r}_{n} \rightarrow \mathbf{r}_{n+1})  =
   \frac{1}{(2\pi\,\delta t)^{3/2}} \exp \left[ - \frac{\left(
   \mathbf{r}_{n+1} - \mathbf{r}_{n} \right)^2}{2\delta t} \right]\,.
   \]


   Furthermore, to sample the density even better, we can "push" the electrons
   into in the regions of high probability, and "pull" them away from
   the low-probability regions. This will increase the
   acceptance ratios and improve the sampling.

   To do this, we can use the gradient of the probability density

   \[
   \frac{\nabla [ \Psi^2 ]}{\Psi^2} = 2 \frac{\nabla \Psi}{\Psi}\,,
   \]

   and add the so-called drift vector, $\frac{\nabla \Psi}{\Psi}$, so that the numerical scheme becomes a
   drifted diffusion with transition probability:

    \[
   T(\mathbf{r}_{n} \rightarrow \mathbf{r}_{n+1})  =
   \frac{1}{(2\pi\,\delta t)^{3/2}} \exp \left[ - \frac{\left(
   \mathbf{r}_{n+1} - \mathbf{r}_{n} - \delta t\frac{\nabla
   \Psi(\mathbf{r}_n)}{\Psi(\mathbf{r}_n)} \right)^2}{2\,\delta t} \right]\,.
   \]

   The corresponding move is proposed as

   \[
   \mathbf{r}_{n+1} = \mathbf{r}_{n} + \delta t\, \frac{\nabla
   \Psi(\mathbf{r})}{\Psi(\mathbf{r})} + \chi \,,
   \]

   where $\chi$ is a Gaussian random variable with zero mean and
   variance $\delta t$.



   The algorithm of the previous exercise is only slightly modified as:

   1) Evaluate the local energy at $\mathbf{r}_{n}$ and accumulate it
   2) Compute a new position $\mathbf{r'} = \mathbf{r}_n +
      \delta t\, \frac{\nabla \Psi(\mathbf{r})}{\Psi(\mathbf{r})} + \chi$
   3) Evaluate $\Psi(\mathbf{r}')$ and $\frac{\nabla \Psi(\mathbf{r'})}{\Psi(\mathbf{r'})}$ at the new position
   4) Compute the ratio $A = \frac{T(\mathbf{r}' \rightarrow \mathbf{r}_{n}) P(\mathbf{r}')}{T(\mathbf{r}_{n} \rightarrow \mathbf{r}') P(\mathbf{r}_{n})}$
   5) Draw a uniform random number $v \in [0,1]$
   6) if $v \le A$, accept the move : set $\mathbf{r}_{n+1} = \mathbf{r'}$
   7) else, reject the move : set $\mathbf{r}_{n+1} = \mathbf{r}_n$

*** Exercise 1

     #+begin_exercise
     If you use Fortran, copy/paste the ~random_gauss~ function in
     a Fortran file.
     #+end_exercise

     #+begin_exercise
     Write a function to compute the drift vector $\frac{\nabla \Psi(\mathbf{r})}{\Psi(\mathbf{r})}$.
     #+end_exercise
**** Solution                                                     :solution:
    *Julia*
     #+BEGIN_SRC julia :tangle hydrogen.jl
using LinearAlgebra

function drift(a,r)
   ar_inv = -a/norm(r)
   return r * ar_inv
end
     #+END_SRC
*** Exercise 2

    #+begin_exercise
    Modify the previous program to introduce the drift-diffusion scheme.
    (This is a necessary step for the next section on diffusion Monte Carlo).
    #+end_exercise
**** Solution                                                      :solution:
    *Julia*
     #+BEGIN_SRC julia :results output :exports both
using LinearAlgebra
using Distributions

include("hydrogen.jl")
include("qmc_stats.jl")

function MonteCarlo(a,nmax,dt)
    sq_dt = sqrt(dt)

    energy  = 0.
    N_accep = 0

    x_old = rand()*2*dt - dt;
    y_old = rand()*2*dt - dt;
    z_old = rand()*2*dt - dt;
    r_old = [x_old,y_old,z_old];
    psi_old = psi(a,r_old);
    d_old   = drift(a,r_old)
    d2_old  = dot(d_old,d_old)

    # Normal distribution
    d = Normal(0.0,1.0);

    for istep in range(1,nmax)
        chi = rand(d,3);

        energy += e_loc(a,r_old)

        x_new = rand()*2*dt - dt;
        y_new = rand()*2*dt - dt;
        z_new = rand()*2*dt - dt;
        r_new = [x_new,y_new,z_new];
        r_new   = r_old + dt * d_old + sq_dt * chi
        d_new   = drift(a,r_new)
        d2_new  = dot(d_new,d_new)
        psi_new = psi(a,r_new)

        # Metropolis
        prod    = dot((d_new + d_old), (r_new - r_old))
        argexpo = 0.5 * (d2_new - d2_old)*dt + prod

        q = psi_new / psi_old
        q = exp(-argexpo) * q*q

        if rand() <= q
            N_accep += 1

            r_old   = r_new
            d_old   = d_new
            d2_old  = d2_new
            psi_old = psi_new
        end
    end

    return([energy/nmax, N_accep/nmax])
end


# Run simulation
a    = 1.2
nmax = 100000
dt   = 1.0

X0 = foldl(hcat,1:30 .|> x->MonteCarlo(a,nmax,dt))

# Energy
X = X0[1,:];
E, deltaE = ave_error(X)
print("E = $(E) +/- $(deltaE)")


# Acceptance rate
X = X0[2,:];
A, deltaA = ave_error(X)
print("A = $(A) +/- $(deltaA)")
     #+END_SRC

     #+RESULTS:
     : E = -0.48031432448301836 +/- 0.0004849708507147206
     : A = 0.6209063333333334 +/- 0.00043997556968053034
* Diffusion Monte Carlo

  As we have seen, Variational Monte Carlo is a powerful method to
  compute integrals in large dimensions. It is often used in cases
  where the expression of the wave function is such that the integrals
  can't be evaluated (multi-centered Slater-type orbitals, correlation
  factors, etc).

  Diffusion Monte Carlo is different. It goes beyond the computation
  of the integrals associated with an input wave function, and aims at
  finding a near-exact numerical solution to the Schrödinger equation.

** Schrödinger equation in imaginary time

    Consider the time-dependent Schrödinger equation:

    \[
    i\frac{\partial \Psi(\mathbf{r},t)}{\partial t} = (\hat{H} -E_{\rm ref}) \Psi(\mathbf{r},t)\,.
    \]

    where we introduced a shift in the energy, $E_{\rm ref}$, for reasons which will become apparent below.

    We can expand a given starting wave function, $\Psi(\mathbf{r},0)$, in the basis of the eigenstates
    of the time-independent Hamiltonian, $\Phi_k$, with energies $E_k$:

    \[
    \Psi(\mathbf{r},0) = \sum_k a_k\, \Phi_k(\mathbf{r}).
    \]

    The solution of the Schrödinger equation at time $t$ is

    \[
    \Psi(\mathbf{r},t) = \sum_k a_k \exp \left( -i\, (E_k-E_{\rm ref})\, t \right) \Phi_k(\mathbf{r}).
    \]

    Now, if we replace the time variable $t$ by an imaginary time variable
    $\tau=i\,t$, we obtain

    \[
    -\frac{\partial \psi(\mathbf{r}, \tau)}{\partial \tau} = (\hat{H} -E_{\rm ref}) \psi(\mathbf{r}, \tau)
    \]

    where $\psi(\mathbf{r},\tau) = \Psi(\mathbf{r},-i\,\tau)$
    and

    \begin{eqnarray*}
    \psi(\mathbf{r},\tau) &=& \sum_k a_k \exp( -(E_k-E_{\rm ref})\, \tau) \Phi_k(\mathbf{r})\\
                          &=& \exp(-(E_0-E_{\rm ref})\, \tau)\sum_k a_k \exp( -(E_k-E_0)\, \tau) \Phi_k(\mathbf{r})\,.
    \end{eqnarray*}

    For large positive values of $\tau$, $\psi$ is dominated by the
    $k=0$ term, namely, the lowest eigenstate. If we adjust $E_{\rm ref}$ to the running estimate of $E_0$,
    we can expect that simulating the differential equation in
    imaginary time will converge to the exact ground state of the
    system.

** Relation to diffusion

    The [[https://en.wikipedia.org/wiki/Diffusion_equation][diffusion equation]] of particles is given by

    \[
    \frac{\partial \psi(\mathbf{r},t)}{\partial t} = D\, \Delta \psi(\mathbf{r},t)
    \]

    where $D$ is the diffusion coefficient. When the imaginary-time
    Schrödinger equation is written in terms of the kinetic energy and
    potential,

    \[
    \frac{\partial \psi(\mathbf{r}, \tau)}{\partial \tau} =
    \left(\frac{1}{2}\Delta - [V(\mathbf{r}) -E_{\rm ref}]\right) \psi(\mathbf{r}, \tau)\,,
    \]

    it can be identified as the combination of:
    - a diffusion equation (Laplacian)
    - an equation whose solution is an exponential (potential)

    The diffusion equation can be simulated by a Brownian motion:

    \[ \mathbf{r}_{n+1} = \mathbf{r}_{n} + \sqrt{\delta t}\, \chi \]

    where $\chi$ is a Gaussian random variable, and the potential term
    can be simulated by creating or destroying particles over time (a
    so-called branching process) or by simply considering it as a
    cumulative multiplicative weight along the diffusion trajectory
    (pure Diffusion Monte Carlo):

   \[
    \prod_i \exp \left( - (V(\mathbf{r}_i) - E_{\text{ref}}) \delta t \right).
   \]


    We note that the ground-state wave function of a Fermionic system is
    antisymmetric and changes sign. Therefore, its interpretation as a probability
    distribution is somewhat problematic. In fact, mathematically, since
    the Bosonic ground state is lower in energy than the Fermionic one, for
    large $\tau$, the system will evolve towards the Bosonic solution.

    For the systems you will study, this is not an issue:

    - Hydrogen atom: You only have one electron!
    - Two-electron system ($H_2$ or He): The ground-wave function is
      antisymmetric in the spin variables but symmetric in the space ones.

    Therefore, in both cases, you are dealing with a "Bosonic" ground state.

** Importance sampling

    In a molecular system, the potential is far from being constant
    and, in fact, diverges at the inter-particle coalescence points. Hence,
    it results in very large fluctuations of the weight associated with
    the potential, making the calculations impossible in practice.
    Fortunately, if we multiply the Schrödinger equation by a chosen
    /trial wave function/ $\Psi_T(\mathbf{r})$ (Hartree-Fock, Kohn-Sham
    determinant, CI wave function, /etc/), one obtains

  \[
    -\frac{\partial \psi(\mathbf{r},\tau)}{\partial \tau} \Psi_T(\mathbf{r}) =
    \left[ -\frac{1}{2} \Delta \psi(\mathbf{r},\tau) + V(\mathbf{r}) \psi(\mathbf{r},\tau) \right] \Psi_T(\mathbf{r})
  \]

  Defining $\Pi(\mathbf{r},\tau) = \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r})$, (see appendix for details)

  \[
  -\frac{\partial \Pi(\mathbf{r},\tau)}{\partial \tau}
  = -\frac{1}{2} \Delta \Pi(\mathbf{r},\tau) +
  \nabla \left[ \Pi(\mathbf{r},\tau) \frac{\nabla \Psi_T(\mathbf{r})}{\Psi_T(\mathbf{r})}
  \right] + (E_L(\mathbf{r})-E_{\rm ref})\Pi(\mathbf{r},\tau)
  \]

  The new "kinetic energy" can be simulated by the drift-diffusion
  scheme presented in the previous section (VMC).
  The new "potential" is the local energy, which has smaller fluctuations
  when $\Psi_T$ gets closer to the exact wave function.
  This term can be simulated by
   \[
    \prod_i \exp \left( - (E_L(\mathbf{r}_i) - E_{\text{ref}}) \delta t \right).
   \]
  where $E_{\rm ref}$ is the constant we had introduced above, which is adjusted to
  an estimate of the average energy to keep the weights close to one.

  This equation generates the /N/-electron density $\Pi$, which is the
  product of the ground state solution with the trial wave
  function. You may then ask: how can we compute the total energy of
  the system?

  To this aim, we use the /mixed estimator/ of the energy:

  \begin{eqnarray*}
   E(\tau)  &=&  \frac{\langle \psi(\tau) | \hat{H} | \Psi_T \rangle}{\langle \psi(\tau) | \Psi_T \rangle}\\
            &=& \frac{\int \psi(\mathbf{r},\tau) \hat{H} \Psi_T(\mathbf{r}) d\mathbf{r}}
                {\int \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) d\mathbf{r}} \\
            &=& \frac{\int \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) E_L(\mathbf{r}) d\mathbf{r}}
                {\int \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) d\mathbf{r}} \,.
   \end{eqnarray*}

   For large $\tau$, we have that

   \[
   \Pi(\mathbf{r},\tau) =\psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) \rightarrow \Phi_0(\mathbf{r}) \Psi_T(\mathbf{r})\,,
   \]

   and, using that $\hat{H}$ is Hermitian and that $\Phi_0$ is an
   eigenstate of the Hamiltonian, we obtain for large $\tau$

   \[
   E(\tau) = \frac{\langle \psi_\tau | \hat{H} | \Psi_T \rangle}
            {\langle  \psi_\tau | \Psi_T \rangle}
     = \frac{\langle \Psi_T | \hat{H} | \psi_\tau \rangle}
            {\langle  \Psi_T | \psi_\tau \rangle}
     \rightarrow E_0 \frac{\langle  \Psi_T | \Phi_0 \rangle}
            {\langle  \Psi_T | \Phi_0 \rangle}
     = E_0
   \]

   Therefore, we can compute the energy within DMC by generating the
   density $\Pi$ with random walks, and simply averaging the local
   energies computed with the trial wave function.

*** Appendix : Details of the Derivation

  \[
    -\frac{\partial \psi(\mathbf{r},\tau)}{\partial \tau} \Psi_T(\mathbf{r}) =
    \left[ -\frac{1}{2} \Delta \psi(\mathbf{r},\tau) + V(\mathbf{r}) \psi(\mathbf{r},\tau) \right] \Psi_T(\mathbf{r})
  \]

  \[
  -\frac{\partial \big[ \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) \big]}{\partial \tau}
  = -\frac{1}{2} \Big( \Delta \big[
  \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) \big] -
  \psi(\mathbf{r},\tau) \Delta \Psi_T(\mathbf{r}) - 2
  \nabla \psi(\mathbf{r},\tau) \nabla \Psi_T(\mathbf{r}) \Big) + V(\mathbf{r}) \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r})
  \]

  \[
  -\frac{\partial \big[ \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) \big]}{\partial \tau}
  = -\frac{1}{2} \Delta \big[\psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) \big] +
     \frac{1}{2} \psi(\mathbf{r},\tau) \Delta \Psi_T(\mathbf{r}) +
  \Psi_T(\mathbf{r})\nabla \psi(\mathbf{r},\tau) \frac{\nabla \Psi_T(\mathbf{r})}{\Psi_T(\mathbf{r})} + V(\mathbf{r}) \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r})
  \]

  \[
  -\frac{\partial \big[ \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) \big]}{\partial \tau}
  = -\frac{1}{2} \Delta \big[\psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) \big] +
                 \psi(\mathbf{r},\tau) \Delta \Psi_T(\mathbf{r}) +
  \Psi_T(\mathbf{r})\nabla \psi(\mathbf{r},\tau) \frac{\nabla \Psi_T(\mathbf{r})}{\Psi_T(\mathbf{r})} + E_L(\mathbf{r}) \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r})
  \]
  \[
  -\frac{\partial \big[ \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) \big]}{\partial \tau}
  = -\frac{1}{2} \Delta \big[\psi(\mathbf{r},\tau) \Psi_T(\mathbf{r}) \big] +
  \nabla \left[ \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r})
  \frac{\nabla \Psi_T(\mathbf{r})}{\Psi_T(\mathbf{r})}
  \right] + E_L(\mathbf{r}) \psi(\mathbf{r},\tau) \Psi_T(\mathbf{r})
  \]

  Defining $\Pi(\mathbf{r},t) = \psi(\mathbf{r},\tau)
  \Psi_T(\mathbf{r})$,

  \[
  -\frac{\partial \Pi(\mathbf{r},\tau)}{\partial \tau}
  = -\frac{1}{2} \Delta \Pi(\mathbf{r},\tau) +
  \nabla \left[ \Pi(\mathbf{r},\tau) \frac{\nabla \Psi_T(\mathbf{r})}{\Psi_T(\mathbf{r})}
  \right] + E_L(\mathbf{r}) \Pi(\mathbf{r},\tau)
  \]

** Pure Diffusion Monte Carlo

   Instead of having a variable number of particles to simulate the
   branching process as in the /Diffusion Monte Carlo/ (DMC) algorithm, we
   use variant called /pure Diffusion Monte Carlo/ (PDMC) where
   the potential term is considered as a cumulative product of weights:

   \begin{eqnarray*}
   W(\mathbf{r}_n, \tau) = \prod_{i=1}^{n} \exp \left( -\delta t\,
   (E_L(\mathbf{r}_i) - E_{\text{ref}}) \right) =
   \prod_{i=1}^{n} w(\mathbf{r}_i)
   \end{eqnarray*}

   where $\mathbf{r}_i$ are the coordinates along the trajectory and
   we introduced a /time-step variable/ $\delta t$ to discretize the
   integral.

   The PDMC algorithm is less stable than the DMC algorithm: it
   requires to have a value of $E_\text{ref}$ which is close to the
   fixed-node energy, and a good trial wave function. Moreover, we
   can't let $\tau$ become too large because the weight whether
   explode or vanish: we need to have a fixed value of $\tau$
   (projection time).
   The big advantage of PDMC is that it is rather simple to implement
   starting from a VMC code:

   0) Start with $W(\mathbf{r}_0)=1, \tau_0 = 0$
   1) Evaluate the local energy at $\mathbf{r}_{n}$
   2) Compute the contribution to the weight $w(\mathbf{r}_n) =
      \exp(-\delta t(E_L(\mathbf{r}_n)-E_\text{ref}))$
   3) Update $W(\mathbf{r}_{n}) = W(\mathbf{r}_{n-1}) \times w(\mathbf{r}_n)$
   4) Accumulate the weighted energy $W(\mathbf{r}_n) \times
      E_L(\mathbf{r}_n)$,
      and the weight $W(\mathbf{r}_n)$ for the normalization
   5) Update $\tau_n = \tau_{n-1} + \delta t$
   6) If $\tau_{n} > \tau_\text{max}$ ($\tau_\text{max}$ is an input parameter), the long projection time has
      been reached and we can start an new trajectory from the current
      position: reset $W(r_n) = 1$ and $\tau_n
      = 0$
   7) Compute a new position $\mathbf{r'} = \mathbf{r}_n +
      \delta t\, \frac{\nabla \Psi(\mathbf{r})}{\Psi(\mathbf{r})} + \chi$
   8) Evaluate $\Psi(\mathbf{r}')$ and $\frac{\nabla \Psi(\mathbf{r'})}{\Psi(\mathbf{r'})}$ at the new position
   9) Compute the ratio $A = \frac{T(\mathbf{r}' \rightarrow \mathbf{r}_{n}) P(\mathbf{r}')}{T(\mathbf{r}_{n} \rightarrow \mathbf{r}') P(\mathbf{r}_{n})}$
  10) Draw a uniform random number $v \in [0,1]$
  11) if $v \le A$, accept the move : set $\mathbf{r}_{n+1} = \mathbf{r'}$
  12) else, reject the move : set $\mathbf{r}_{n+1} = \mathbf{r}_n$


   Some comments are needed:

   - You estimate the energy as

     \begin{eqnarray*}
     E = \frac{\sum_{k=1}^{N_{\rm MC}} E_L(\mathbf{r}_k) W(\mathbf{r}_k, k\delta t)}{\sum_{k=1}^{N_{\rm MC}} W(\mathbf{r}_k, k\delta t)}
     \end{eqnarray*}

   - The result will be affected by a time-step error
     (the finite size of $\delta t$) due to the discretization of the
     integral, and one has in principle to extrapolate to the limit
     $\delta t \rightarrow 0$. This amounts to fitting the energy
     computed for multiple values of $\delta t$.

     Here, you will be using a small enough time-step and you should not worry about the extrapolation.
   - The accept/reject step (steps 9-12 in the algorithm) is in principle not needed for the correctness of
     the DMC algorithm. However, its use reduces significantly the time-step error.


** Hydrogen atom
  :PROPERTIES:
  :header-args:julia: :tangle pdmc.jl
  :END:

*** Exercise

     #+begin_exercise
     Modify the Metropolis VMC program into a PDMC program.
     In the limit $\delta t \rightarrow 0$, you should recover the exact
     energy of H for any value of $a$, as long as the simulation is stable.
     We choose here a time step of 0.05 a.u. and a fixed projection
     time $\tau_{\text{max}}$ =100 a.u.
     #+end_exercise
**** Solution                                                      :solution:

     *Julia*
     #+BEGIN_SRC julia :results output
using LinearAlgebra
using Distributions

include("hydrogen.jl")
include("qmc_stats.jl")

function MonteCarlo(a, nmax, dt, tau, Eref)
    sq_dt = sqrt(dt)

    energy  = 0.
    N_accep = 0
    normalization = 0.

    w           = 1.
    tau_current = 0.

    x_old = rand()*2*dt - dt;
    y_old = rand()*2*dt - dt;
    z_old = rand()*2*dt - dt;
    r_old = [x_old,y_old,z_old];
    d_old   = drift(a,r_old)
    d2_old  = dot(d_old,d_old)
    psi_old = psi(a,r_old)

    # Normal distribution
    d = Normal(0.0,1.0);

    for istep in range(1,nmax)
        el = e_loc(a,r_old)
        w *= exp(-dt*(el - Eref))

        normalization += w
        energy        += w * el

        tau_current += dt

        # Reset when tau is reached
        if tau_current > tau
            w           = 1.
            tau_current = 0.
        end

        chi = rand(d,3);

        x_new = rand()*2*dt - dt;
        y_new = rand()*2*dt - dt;
        z_new = rand()*2*dt - dt;
        r_new = [x_new,y_new,z_new];
        r_new = r_old + dt * d_old + sq_dt * chi
        d_new = drift(a,r_new)
        d2_new = dot(d_new,d_new)
        psi_new = psi(a,r_new)

        # Metropolis
        prod = dot((d_new + d_old), (r_new - r_old))
        argexpo = 0.5 * (d2_new - d2_old)*dt + prod

        q = psi_new / psi_old
        q = exp(-argexpo) * q*q

        if rand() <= q
            N_accep += 1
            r_old   = r_new
            d_old   = d_new
            d2_old  = d2_new
            psi_old = psi_new
        end
    end

    return([energy/normalization, N_accep/nmax])
end


# Run simulation
a     = 1.2
nmax  = 100000
dt    = 0.05
tau   = 100.
E_ref = -0.5

X0 = foldl(hcat,1:30 .|> x->MonteCarlo(a,nmax,dt,tau,E_ref))

# Energy
X = X0[1,:];
E, deltaE = ave_error(X)
print("E = $(E) +/- $(deltaE)")

# Acceptance rate
X = X0[2,:];
A, deltaA = ave_error(X)
print("A = $(A) +/- $(deltaA)")
     #+END_SRC

     #+RESULTS:
     : E = -0.4990976236303405 +/- 0.0007252894721434303
     : A = 0.9896489999999999 +/- 6.907658584964676e-5

* Project

  Change your PDMC code for one of the following:
  - the Helium atom, or
  - the H_2 molecule at $R$ =1.401 bohr.

  And compute the ground state energy.
